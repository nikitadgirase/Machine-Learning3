{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7438dcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.,   0.,  80.,  64.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,  80.,   0.,  80.],\n",
       "       [ 64.,   0.,   0.,  64.,   0., 100.,   0.],\n",
       "       [ 64.,   0.,  80.,   0.,  80.,   0.,   0.],\n",
       "       [  0.,  64.,   0.,  64.,   0., 100.,  80.],\n",
       "       [  0.,   0.,  80.,   0.,  80.,  99.,  80.],\n",
       "       [  0.,  64.,   0.,   0.,  80., 100.,   0.]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import copy \n",
    "\n",
    "# Environemnts \n",
    "rewards = np.array([[-float('inf'), -float('inf'), 0, 0, -float('inf'), -float('inf'), -float('inf')],\n",
    "           [-float('inf'), -float('inf'), -float('inf'), -float('inf'), 0, -float('inf'), 0],\n",
    "           [0, -float('inf'), -float('inf'), 0, -float('inf'), 100, -float('inf')],\n",
    "           [0, -float('inf'), 0, -float('inf'), 0, -float('inf'), -float('inf')],\n",
    "           [-float('inf'), 0, -float('inf'), 0, -float('inf'), 100, 0],\n",
    "           [-float('inf'), -float('inf'), 0, -float('inf'), 0, 100, 0],\n",
    "           [-float('inf'), 0, -float('inf'), -float('inf'), 0, 100, -float('inf')]])\n",
    "\n",
    "# Parameters\n",
    "gamma = 0.8\n",
    "alpha = 0.01\n",
    "num_episode = 50000\n",
    "min_difference = 1e-3\n",
    "goal_state = 5\n",
    "\n",
    "def QLearning(rewards, goal_state=None, gamma=0.99, alpha=0.01, num_episode=1000, min_difference=1e-5):\n",
    "    \"\"\" \n",
    "    Run Q-learning loop for num_episode iterations or till difference between Q is below min_difference.\n",
    "    \"\"\"\n",
    "    Q = np.zeros(rewards.shape)\n",
    "    all_states = np.arange(len(rewards)) \n",
    "    for i in range(num_episode):\n",
    "        Q_old = copy.deepcopy(Q)\n",
    "        # initialize state\n",
    "        initial_state = np.random.choice(all_states)\n",
    "        action = np.random.choice(np.where(rewards[initial_state] != -float('inf'))[0])\n",
    "        Q[initial_state][action] = Q[initial_state][action] + alpha * (rewards[initial_state][action] + gamma * np.max(Q[action]) - Q[initial_state][action])\n",
    "        cur_state = action\n",
    "        # loop for each step of episode, until reaching goal state\n",
    "        while cur_state != goal_state:\n",
    "            # choose action form states using policy derived from Q\n",
    "            action = np.random.choice(np.where(rewards[cur_state] != -float('inf'))[0])\n",
    "            Q[cur_state][action] = Q[cur_state][action] + alpha * (rewards[cur_state][action] + gamma * np.max(Q[action]) - Q[cur_state][action])\n",
    "            cur_state = action\n",
    "\n",
    "        # break the loop if converge\n",
    "        diff = np.sum(Q - Q_old)\n",
    "        if diff < min_difference:\n",
    "            break\n",
    "    return np.around(Q/np.max(Q)*100)\n",
    "            \n",
    "\n",
    "Q = QLearning(rewards, goal_state=goal_state, gamma=gamma, alpha=alpha, num_episode=num_episode, min_difference=min_difference)           \n",
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddde960",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
